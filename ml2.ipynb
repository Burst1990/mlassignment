{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.model_selection as skm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Headers =(['Age','Gender','Chest_Pain_Type','Resting_Blood_Pressure','Serum_Cholesterol_mg/dl','Fasting_Blood_Sugar','Resting_electrocardiographic_results','Maximum_Heart_Rate','Exercise_Induced_Angina','ST_Depression_induced','Slope_of_Peak_Exercise','Major_Vessels','Thal','Classification'])\n",
    "df = pd.read_csv(\"heart-disease.data\",names = Headers, )#read in the data and add headers\n",
    "#load data files and include headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #this returns the first 10 rows of data to represent what the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape() #see what the shape of the data is and how many rows and columns the dataset has"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace('?',np.nan)\n",
    "#replace the '?' found in the data with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf='converted_data')\n",
    "df = pd.read_csv(\"converted_data\",index_col=0,dtype=float)\n",
    "#save converted data with new NaN values and convert all columns to float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "#find the amount of missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Major_Vessels',axis=1,inplace=True)\n",
    "#remove column Major_Vessels as too many missing values to impute accuratly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "corr = data.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr,\n",
    "    vmin=-1,vmax=1,center=0,\n",
    "    cmap=sns.diverging_palette(20,220,n=200),\n",
    "    square=True\n",
    "    )\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ")\n",
    "#heat map to show the columns with the least correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Maximum_Heart_Rate',axis=1,inplace=True)\n",
    "df.drop('Serum_Cholesterol_mg/dl',axis=1,inplace=True)\n",
    "#drop columns with least correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df.dropna(subset=['Slope_of_Peak_Exercise','ST_Depression_induced'],how='all',).shape\n",
    "#see how many rows remaining when both these columns contain missing values in same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Slope_of_Peak_Exercise','ST_Depression_induced'],how='all',inplace=True)\n",
    "#drop those instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Thal'].value_counts(dropna=False)\n",
    "#display what the median values of Thal are and that it is nearly split evenly\n",
    "#between normal and reversable defect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Thal'],inplace=True)\n",
    "#drop Thal missing values as would skew data to impute all NaN values with either 3 or 7 \n",
    "#thal in the heatmap was shown to have a high correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fasting_Blood_Sugar'].value_counts(dropna=False)\n",
    "#display what the median values of Fasting_Blood_Sugar and decide to impute 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fasting_Blood_Sugar'].fillna(value=0.0,inplace=True)\n",
    "#impute false(0.0) for high blood sugar for 25 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Slope_of_Peak_Exercise'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Slope_of_Peak_Exercise'].fillna(value=2.0,inplace=True)\n",
    "#impute flat(2.0) for slope of peak exercise for 42 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resting_Blood_Pressure'].describe()\n",
    "#find the mean for resting blood pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Resting_Blood_Pressure'].fillna(value=132.64,inplace=True)\n",
    "#impute mean RBP into the missing record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "#no more missing records!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "#427 records remain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf KNearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['Classification'],axis=1))\n",
    "y = np.array(df['Classification'])\n",
    "\n",
    "X_train, X_test, y_train,y_test = skm.train_test_split(X,y,test_size=0.3,)\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"accuracy\",metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\", metrics.confusion_matrix(y_test,y_pred))\n",
    "print(\"\\n\",metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My KNearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDistance(x1,x2):\n",
    "    return np.sqrt(sum((x1-x2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self,k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        \n",
    "    def predict(self,X):\n",
    "        predicted_labels = [self.predictSingle(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "    \n",
    "    def predictSingle(self,x):\n",
    "        distances = [euclideanDistance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_n_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_n_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "clf = KNN(k=3)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy1 = sum(predictions == y_test)/len(y_test)\n",
    "print(accuracy1)\n",
    "\n",
    "\n",
    "print(\"\\n\", metrics.confusion_matrix(y_test,predictions))\n",
    "print(\"\\n\",metrics.classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAYBE USE LATER OR DELETE\n",
    "def KNN(data,label,k=3):\n",
    "    if len(data)>=k:\n",
    "        print('uh oh')\n",
    "    distances[]\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclidean_distance =   \n",
    "    return result\n",
    "\n",
    "print (full_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def Dataset(filename,split,trainingSet=[],testSet=[]):\n",
    "    full_data = filename.values.tolist()\n",
    "    for x in range(len(full_data)):\n",
    "        for y in range(4):\n",
    "            full_data[x][y]=float(full_data[x][y])\n",
    "            if random.random()<split:\n",
    "                trainingSet.append(full_data[x])\n",
    "            else:\n",
    "                testSet.append(full_data[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainingSet=[]\n",
    "testSet=[]\n",
    "Dataset(df,0.2,trainingSet,testSet)\n",
    "print('train  ' + repr(len(trainingSet)))\n",
    "print('test  ' + repr(len(testSet)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def euclideanDistance1(train,test,length):\n",
    "    eDistance = 0\n",
    "    for x in range(len(train)):\n",
    "        eDistance += (train[x] -test[x])**2\n",
    "    return sqrt(eDistance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def Neighbors(trainingSet,testSet,k):\n",
    "    edistances = []\n",
    "    length = len(testSet)-1\n",
    "    for x in range(len(trainingSet)):\n",
    "        dist =euclideanDistance(testSet,trainingSet[x],length)\n",
    "        edistances.append((trainingSet[x],dist))\n",
    "    edistances.sort(key=operator.itemgetter(1))\n",
    "    neighbors = []\n",
    "    for x in range(k):\n",
    "        neighbors.append(edistances[x][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "neighbors=Neighbors(trainingSet,testSet,2)\n",
    "print(neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['Classification'],axis=1))\n",
    "y = np.array(df['Classification'])\n",
    "\n",
    "X_train, X_test, y_train,y_test = skm.train_test_split(X,y,test_size=0.3,)\n",
    "\n",
    "GaussianNaiveBayes = GaussianNB()\n",
    "\n",
    "GaussianNaiveBayes = GaussianNaiveBayes.fit(X_train, y_train)\n",
    "y_pred = GaussianNaiveBayes.predict(X_test)\n",
    "    \n",
    "print(\"accuracy\",metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\", metrics.confusion_matrix(y_test,y_pred))\n",
    "print(\"\\n\",metrics.classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd dataset with all null values dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"converted_data\",index_col=0,dtype=float)\n",
    "df2.dropna(inplace=True)\n",
    "#introduce second dataset and drop all null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf KNearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(df2.drop(['Classification'],axis=1))\n",
    "y1 = np.array(df2['Classification'])\n",
    "\n",
    "\n",
    "X_train1, X_test1, y_train1,y_test1 = skm.train_test_split(X1,y1,test_size=.2)\n",
    "\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "classifier.fit(X_train1, y_train1)\n",
    "\n",
    "y_pred1 = classifier.predict(X_test1)\n",
    "\n",
    "\n",
    "print(\"accuracy\",metrics.accuracy_score(y_test1,y_pred1))\n",
    "print(\"\\n\", metrics.confusion_matrix(y_test1,y_pred1))\n",
    "print(\"\\n\",metrics.classification_report(y_test1,y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My KNearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self,k=3):\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.X_train=X\n",
    "        self.y_train=y\n",
    "        \n",
    "    def predict(self,X):\n",
    "        predicted_labels = [self.predictSingle(x) for x in X]\n",
    "        return np.array(predicted_labels)\n",
    "    \n",
    "    def predictSingle(self,x):\n",
    "        distances = [euclideanDistance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_n_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_n_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "clf = KNN(k=3)\n",
    "clf.fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy1 = sum(predictions == y_test)/len(y_test)\n",
    "print(accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off the shelf Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['Classification'],axis=1))\n",
    "y = np.array(df['Classification'])\n",
    "\n",
    "X_train, X_test, y_train,y_test = skm.train_test_split(X,y,test_size=0.3,)\n",
    "\n",
    "GaussianNaiveBayes = GaussianNB()\n",
    "\n",
    "GaussianNaiveBayes = GaussianNaiveBayes.fit(X_train, y_train)\n",
    "y_pred = GaussianNaiveBayes.predict(X_test)\n",
    "    \n",
    "print(\"accuracy\",metrics.accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\", metrics.confusion_matrix(y_test,y_pred))\n",
    "print(\"\\n\",metrics.classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
